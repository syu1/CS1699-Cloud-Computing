{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "| pk|rnum|\n",
      "+---+----+\n",
      "|  0|  83|\n",
      "|  1|  68|\n",
      "|  2|  40|\n",
      "|  3|  52|\n",
      "|  4|   2|\n",
      "|  5|  47|\n",
      "|  6|  59|\n",
      "|  7|  61|\n",
      "|  8|  28|\n",
      "|  9|  40|\n",
      "| 10|  20|\n",
      "| 11|  77|\n",
      "| 12|  35|\n",
      "| 13|  66|\n",
      "| 14|  83|\n",
      "| 15|  83|\n",
      "| 16|  30|\n",
      "| 17|  38|\n",
      "| 18|  10|\n",
      "| 19|  53|\n",
      "+---+----+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-------+------------------+\n",
      "|summary|              rnum|\n",
      "+-------+------------------+\n",
      "|  count|               100|\n",
      "|   mean|             52.48|\n",
      "| stddev|28.539350823985913|\n",
      "|    min|                 2|\n",
      "|    max|               101|\n",
      "+-------+------------------+\n",
      "\n",
      "Variance is STDEV squared. Current stdv for 100 nums is 28.539350823985913\n",
      "Calculated Variance is 814.4945454545455\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import explode, col, udf, mean as _mean, stddev as _stddev\n",
    "from pyspark.sql.types import *\n",
    "sc = SparkContext.getOrCreate()\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "# mock up sample dataframe\n",
    "# initalize random sample dataframe\n",
    "my_list = []\n",
    "\n",
    "def random_nums():\n",
    "    for i in range(100):\n",
    "        my_ran = random.randint(1,101)\n",
    "        \n",
    "        my_list.append((i,my_ran))\n",
    "\n",
    "random_nums()\n",
    "rdd = sc.parallelize(my_list)\n",
    "rans = rdd.map(lambda x: Row(pk=x[0], rnum=int(x[1])))\n",
    "my_schema = spark.createDataFrame(rans)\n",
    "my_schema.show()\n",
    "my_schema2 = my_schema.describe(\"rnum\")\n",
    "\n",
    "my_schema2.show()\n",
    "my_schema2=my_schema2.rdd.take(3)\n",
    "\n",
    "my_stdv = my_schema2[2]\n",
    "my_stdv.__getattr__(\"summary\")\n",
    "final_stv= my_stdv.__getattr__(\"rnum\")\n",
    "vari = float(final_stv)*float(final_stv)\n",
    "print(f\"Variance is STDEV squared. Current stdv for 100 nums is {final_stv}\")\n",
    "print(f\"Calculated Variance is {vari}\")\n",
    "\n",
    "\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
